分布式任务调度框架技术选型，对比了比较知名的开源框架。
包括xxl-job、elastic-job、powerjob、saturn、tbschedule，由于版本较老，可能信息存在问题。
如果对xxl-job有任何问题，可以留言。


|  | xxl-job | elastic-job | powerjob | saturn | tbschedule |
| --- | --- | --- | --- | --- | --- |
| 集群部署 | DB、登录账号配置一致，集群机器时钟一致 | 依赖ZK实现的全局作业注册控制中心 | 最小仅依赖关系型数据库 | Elastic-job的fork版本 | 依赖zk存储调度数据，后端依赖的ZooKeeper用于存储策略、任务、心跳信息数据 |
| 多节点部署时任务不能重复执行 | 采用DB方式进行任务注册发现 | 将任务拆分为任务项，各个服务分别执行。一旦由服务器的加入或下线，elastic-job将保留本次任务执行不变的情况下， 在下次任务开始前触发任务重分片。 | 不详 | 同elastic-job | 配置执行机多线程且划分多任务子项后，各任务子项均衡分配到所有执行机，各执行机均执行，多线程数据一致性协调由任务项参数区分。 |
| 日志可追溯 | 日志查询界面 | 基于关系型数据库的事件订阅方式记录事件 | 支持在线日志功能 | 支持查看，同时支持jstack和gc log备份到executor日志目录 | 不支持 |
| 监控告警 | 任务调度失败时邮件通知 | 通过事件订阅方式自行实现 | 邮件告警，提供接口允许开发者扩展 | 支持，需要自己接口实现executor | 支持 |
| 弹性扩容缩容 | 服务器超出一定数量会给数据库造成一定的压力，一旦有新执行器机器上线或者下线，下次调度时将会重新分配任务 | 通过zk的注册与发现，可以动态的添加服务器，支持水平扩容。运行中的作业服务器崩溃或新增，作业框架将在下次作业执行前重新分片，不影响当前作业执行 | 支持无限的水平扩展 | 通过zk实现服务的注册、协调及控制能支持容器化技术进行executor扩容和减容 | 动态的服务扩容和资源回收。在ZooKeeper中创建唯一性路径（临时节点），新上线的服务器会和ZooKeeper保持长连接，当通信断开后，节点会自动摘除。 |
| 支持并行调度 | 任务路由策略选择”分片广播“情况下，一次任务调度会广播触发集群中所有执行器执行一次任务，根据分片参数处理分片任务 | 任务分片，由分布式的服务器并行执行各自分配到的分片项 | MapReduce 动态分片，支持并行调度 | 同elastic-job | 配置执行机多线程且划分多任务子项后，各任务子项均衡分配到所有执行机，各执行机均执行，多线程数据一致性协调由任务项参数区分。 |
| 动态分片策略 | 以执行器为维度对分片广播任务进行分片，支持动态扩容执行器集群从而动态增加分片数量 | 支持多种分片策略、可自定义 | MapReduce 动态分片 | 人工指定和自动平均策略结合 | 多种分片方法 |
| 失败处理策略 | 失败告警、失败重试、故障转移 | 失败转移、被错过的作业重触发 | 任务执行失败后，可根据配置的重试策略完成重试 | 异常检测和自动失败转移、超时报警、超时强杀 | 失败转移 |
| DAG任务 | 支持简单的子任务和任务依赖，不支持完整的DAG任务 | 计划支持 | 支持在线配置任务依赖关系，可视化编排，同时还支持上下游任务间的数据传递 | 支持作业编排,作业编排将作业形成一个有向无环图，按照图的顺序依次调用。 | 支持在线配置任务依赖关系（DAG），以可视化的方式对任务进行编排，同时还支持上下游任务间的数据传递。 |
| 容器部署 | 支持Docker | 计划支持 | 支持 | 支持Docker |  |
| 触发方式 | 时间、事件触发 | 时间触发 | 时间触发 | 时间、事件触发 | 时间触发 |
| 文档 | 完善 | 比较完善 | 比较完善 | 比较完善 | 少 |
| 缺点 | 调度中心通过获取 DB锁来保证集群中执行任务的唯一性， 短任务多时性能不好；仅支持静态分片，无法很好的完成复杂任务的计算；不支持DAG | 需要引入zookeeper , mesos, 增加系统复杂度, 学习成本较高 | 比较新；性能无上限待验证（可能依赖于数据库性能）；mysql版本官方要求8.x；代码未更新 | 需要引入zookeeper , mesos, 增加系统复杂度, 学习成本较高 | 文档少；不再更新；引入zookeeper |
| 部署运维 | 简单 | 复杂 | 简单 | 复杂 | 复杂 |
| Spring整合 | springboot启动，和spring简单整合 | 与Spring依赖注入无缝整合 | 支持整合 | 支持集成spring、springboot和嵌入式使用 | 支持整合 |
| 更新状态 | 稳定更新 | 稳定更新，最近 | 稳定更新，最近 | 2021/12/12 | 不再更新 |
|  |  |  |  |  |  |

### Elastic-job-lite

- 核心组件：
    - quartz：调度每台机器上的任务（每台机器上的分片任务何时执行）
    - zookeeper：分布式调度中心
    

![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled.png)

- elastic-job-lite 以 zookeeper 作为注册中心
- console 作为控制台和服务端解构，直接操纵 zk 改变 job 的配置信息
- 服务端启动时连接 zk，注册 job，初始化 Scheduler, 进行 leader 选举，分片m按照 job 配置信息调度作业
- 控制中心不支持HA
- Elastic-job弹性扩容缩容
    - 第一台服务器上线触发主服务器选举。主服务器一旦下线，则重新触发选举，选举过程中阻塞，只有主服务器选举完成，才会执行其他任务。
    - 某作业服务器上线时会自动将服务器信息注册到注册中心，下线时会自动更新服务器状态。
    - 主节点选举、服务器上下线、分片总数变更均更新重新分片标记。
    - 定时任务触发时，如需重新分片，则通过主服务器分片，分片过程中阻塞，分片结束后才可执行任务。如分片过程中主服务器下线，则先选举主服务器，再分片。
    - 运行过程中只会标记分片状态，不会重新分片。分片仅可能发生在下次任务触发前。
    - 每次分片都会按服务器IP排序，保证分片结果不会产生较大波动。
    - 实现失效转移功能，在某台服务器执行完毕后主动抓取未分配的分片，并且在某台服务器下线后主动寻找可用的服务器执行任务。

### Saturn

- 资源隔离：对执行节点进行功能划分
- 作业隔离：将需要隔离的作业的负载值设为远超其它作业的值

![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%201.png)

将作业在逻辑上划分为若干个作业分片，通过作业分片调度器将作业分片指派给特定的执行结点。

![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%202.png)

![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%203.png)

上线：执行结点启动，往ZK创建临时结点

上线调度：

- 作业分片调度器监听ZK路径，若判断出有新结点上线
- 作业分片调度器根据算法进行分片调度，并将结果写入ZK
- 作业分片调度器将重新加载指令，写入全部作业的结点

分片加载：

- 执行结点从ZK读取重新加载指令，如果大于0，则重新加载分片
- 作业的全部执行结点会选举出一个leader，并由leader进行分片重载

### tbschedule

![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%204.png)

使用timer而非线程池执行任务调度，存在缺陷

- TimerTask中出现未捕获的异常，影响Timer
- 因为是单线程执行某个任务执行时间过长会影响其他任务的精确度

### powerjob

worker**&server的绑定**

![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%205.png)

![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%206.png)

### Worker获取Server地址

```java
public void init() throws Exception {
   //初始化Akka
    ...
	// 服务发现
	currentServer = ServerDiscoveryService.discovery();
	if (StringUtils.isEmpty(currentServer) && !config.isEnableTestMode()) {
	    throw new RuntimeException("can't find any available server, this worker has been quarantined.");
	}
	log.info("[OhMyWorker] discovery server succeed, current server is {}.", currentServer);
	……
}
```

```java
public static String discovery() {
    if (IP2ADDRESS.isEmpty()) {
				//返回的是在Worker进行初始化的时候，配置的Server地址
        OhMyWorker.getConfig().getServerAddress().forEach(x -> IP2ADDRESS.put(x.split(":")[0], x));
    }

    String result = null;

    // 先对当前机器发起请求
    //1、判断之前是否已确定一个可用的Server，如果之前已经确定了一个Server,就再验证这个Server是否还是可用的
    String currentServer = OhMyWorker.getCurrentServer();
    if (!StringUtils.isEmpty(currentServer)) {
        String ip = currentServer.split(":")[0];
        // 直接请求当前Server的HTTP服务，可以少一次网络开销，减轻Server负担
        String firstServerAddress = IP2ADDRESS.get(ip);
        if (firstServerAddress != null) {
            result = acquire(firstServerAddress);
        }
    }

    //2、如果之前没有可用的Server,依次判断Server数组中的Server，查找出可用的Server地址
    for (String httpServerAddress : OhMyWorker.getConfig().getServerAddress()) {
        if (StringUtils.isEmpty(result)) {
            result = acquire(httpServerAddress);
        }else {
            break;
        }
    }

    //3、如果没有找到可用的Server，说明当前Worker与外界失联，进行错误处理
    if (StringUtils.isEmpty(result)) {
        log.warn("[OmsServerDiscovery] can't find any available server, this worker has been quarantined.");

        // 在 Server 高可用的前提下，连续失败多次，说明该节点与外界失联，Server已经将秒级任务转移到其他Worker，需要杀死本地的任务
        //错误处理
        return null;
    }else {
        // 重置失败次数
        FAILED_COUNT = 0;
        log.debug("[OmsServerDiscovery] current server is {}.", result);
        return result;
    }
}
```

- 任务类型
    - 单机模式
        
        ```java
        @Slf4j
        @Component
        public class StandaloneProcessor implements BasicProcessor { //实现BasicProcessor接口
         
            @Override
            public ProcessResult process(TaskContext context) { //核心触发逻辑
                log.info("简单定时任务-触发！，参数是：{}", context.getJobParams());
                return new ProcessResult(true, context + ": " + true);
            }
        }
        ```
        
    - 广播模式
    
    ![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%207.png)
    
    ```java
    @Slf4j
    @Component
    public class BroadcastProcessorDemo extends BroadcastProcessor { //继承BroadcastProcessor类
     
        @Override
        public ProcessResult preProcess(TaskContext context) throws Exception { //在所有节点广播执行前执行，只会在一台机器执行一次
            log.info("广播前，参数：{}", context.getJobParams());
            return new ProcessResult(true);
        }
     
        @Override
        public ProcessResult process(TaskContext taskContext) throws Exception { //核心逻辑，会广播给所有节点并行处理
            log.info("广播核心逻辑触发！参数：{}", taskContext.getJobParams());
            return new ProcessResult(true);
        }
     
        @Override
        public ProcessResult postProcess(TaskContext context, List<TaskResult> taskResults) throws Exception { //在所有节点广播执行完成后执行，只会在一台机器执行一次
            //通知执行结果，有点类似下面要测试的MapReduce的reduce方法
            log.info("广播任务执行完毕，reduce触发！TaskContext: {}，List<TaskResult>: {}",
                JSONObject.toJSONString(context), JSONObject.toJSONString(taskResults));
            return new ProcessResult(true, "success");
        }
    }
    ```
    
    - Map
    
    ![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%208.png)
    
    ```java
    @Slf4j
    @Component
    public class MapProcessorDemo extends MapProcessor { //继承MapProcessor
     
        private static final int batchSize = 100; //单批发送数据量
        private static final int batchNum = 2; //一共2批，默认上限为200批，再多就要适当调整batchSize大小了
     
        @Override
        public ProcessResult process(TaskContext context) throws Exception {
     
            if (isRootTask()) { //如果是根任务（说明map刚被调度到），则触发任务拆分
                log.info("根任务，需要做任务拆分~");
                List<SubTask> subTasks = Lists.newLinkedList();
                for (int j = 0; j < batchNum; j++) {
                    SubTask subTask = new SubTask();
                    subTask.siteId = j;
                    subTask.itemIds = Lists.newLinkedList();
                    subTasks.add(subTask); //批次入集合
                    for (int i = 0; i < batchSize; i++) { //内部id集合，这里只是举例，实际业务场景可以是从db里获取的业务id集合
                        subTask.itemIds.add(i);
                    }
                }
                return map(subTasks, "MAP_TEST_TASK"); //最后调用map，触发这些批次任务的执行
            } else { //子任务，说明批次已做过拆分，此时被调度到时会触发下方逻辑
                SubTask subTask = (SubTask) context.getSubTask(); //直接从上下文对象里拿到批次对象
                log.info("子任务，拿到的批次实体为：{}", JSON.toJSONString(subTask));
                return new ProcessResult(true, "RESULT:true");
            }
        }
     
        @Getter
        @NoArgsConstructor
        @AllArgsConstructor
        private static class SubTask { //定义批次实体（业务方可自由发挥）
            private Integer siteId; //批次id
            private List<Integer> itemIds; //批次内部所携带的id（可以是我们自己的业务id）
        }
    }
    ```
    
    - MapReduce
    
    ![Untitled](../images/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2033ea7218cae24b0a8a48e7f8bee6199a/Untitled%209.png)
    
    ```java
    @Slf4j
    @Component
    public class MapReduceProcessorDemo extends MapReduceProcessor { //需要继承MapReduceProcessor
     
        private static final int batchSize = 100;
        private static final int batchNum = 2;
     
        @Override
        public ProcessResult process(TaskContext context) { //该方法跟普通map方法实现一致，主要用来拆分子任务和执行子任务
     
            if (isRootTask()) {
                log.info("根任务，需要做任务拆分~");
                List<SubTask> subTasks = Lists.newLinkedList();
                for (int j = 0; j < batchNum; j++) {
                    SubTask subTask = new SubTask();
                    subTask.siteId = j;
                    subTask.itemIds = Lists.newLinkedList();
                    subTasks.add(subTask); //批次入集合
                    for (int i = 0; i < batchSize; i++) {
                        subTask.itemIds.add(i);
                    }
                }
                return map(subTasks, "MAP_TEST_TASK");
            } else {
                SubTask subTask = (SubTask) context.getSubTask();
                log.info("子任务，拿到的批次实体为：{}", JSON.toJSONString(subTask));
                return new ProcessResult(true, "RESULT:true");
            }
        }
     
        @Override
        public ProcessResult reduce(TaskContext context, List<TaskResult> taskResults) { //相比普通map任务，多出reduce方法，这里将两个参数全部打印出来
            log.info("子任务执行完毕，reduce触发！TaskContext: {}，List<TaskResult>: {}",
                JSONObject.toJSONString(context), JSONObject.toJSONString(taskResults));
            return new ProcessResult(true, "RESULT:true");
        }
     
        @Getter
        @NoArgsConstructor
        @AllArgsConstructor
        private static class SubTask {
            private Integer siteId;
            private List<Integer> itemIds;
        }
    }
    ```
    

定时类型&验证

CRON表达式

固定频率

固定延迟